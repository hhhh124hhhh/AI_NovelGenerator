# config_manager.py
# -*- coding: utf-8 -*-
import json
import os
import threading
import sys
import yaml
from typing import Dict, Any
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥é«˜çº§æ—¥å¿—ç³»ç»Ÿ
from advanced_logger import main_logger, llm_logger, embedding_logger, log_llm_request, log_llm_response, log_embedding_request, log_embedding_response

from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter


def is_uv_environment():
    """æ£€æµ‹æ˜¯å¦åœ¨ uv ç¯å¢ƒä¸­è¿è¡Œ"""
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ uv run å‘½ä»¤è¿è¡Œ
    return 'UV' in os.environ or 'uv' in sys.argv[0].lower()


def get_environment_info():
    """è·å–ç¯å¢ƒä¿¡æ¯"""
    info = {
        'is_uv': is_uv_environment(),
        'python_version': sys.version,
        'platform': sys.platform
    }
    return info


def load_config(config_file: str) -> dict:
    """ä»æŒ‡å®šçš„ config_file åŠ è½½é…ç½®ï¼Œæ”¯æŒJSONå’ŒYAMLæ ¼å¼ï¼Œè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤é…ç½®æ–‡ä»¶ã€‚"""
    main_logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")

    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
    if not os.path.exists(config_file):
        main_logger.info("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®")
        create_config(config_file)

    try:
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ ¼å¼
        if config_file.endswith('.yaml') or config_file.endswith('.yml'):
            with open(config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
        else:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        main_logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {}


def replace_env_vars(config: Dict[Any, Any]) -> Dict[Any, Any]:
    """æ›¿æ¢é…ç½®ä¸­çš„ç¯å¢ƒå˜é‡å ä½ç¬¦"""
    if isinstance(config, dict):
        return {k: replace_env_vars(v) for k, v in config.items()}
    elif isinstance(config, list):
        return [replace_env_vars(item) for item in config]
    elif isinstance(config, str):
        # æ›¿æ¢ ${VAR_NAME} æ ¼å¼çš„ç¯å¢ƒå˜é‡
        import re
        def replace_var(match):
            var_name = match.group(1)
            return os.getenv(var_name, match.group(0))  # å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œä¿æŒåŸæ ·
        return re.sub(r'\$\{([^}]+)\}', replace_var, config)
    else:
        return config


# PenBo å¢åŠ äº†åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶å‡½æ•°
def create_config(config_file: str) -> dict:
    """åˆ›å»ºä¸€ä¸ªé»˜è®¤é…ç½®æ–‡ä»¶ã€‚"""
    main_logger.info("åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶")
    config = {
    "last_interface_format": "OpenAI",
    "last_embedding_interface_format": "OpenAI",
    "llm_configs": {
        "DeepSeek V3": {
            "api_key": "${DEEPSEEK_API_KEY}",
            "base_url": "https://api.deepseek.com/v1",
            "model_name": "deepseek-chat",
            "temperature": 0.7,
            "max_tokens": 8192,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "GPT 5": {
            "api_key": "${OPENAI_API_KEY}",
            "base_url": "https://api.openai.com/v1",
            "model_name": "gpt-5",
            "temperature": 0.7,
            "max_tokens": 32768,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "Gemini 2.5 Pro": {
            "api_key": "${GEMINI_API_KEY}",
            "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
            "model_name": "gemini-2.5-pro",
            "temperature": 0.7,
            "max_tokens": 32768,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "æ™ºè°±GLM-4.5": {
            "api_key": "${ZHIPUAI_API_KEY}",
            "base_url": "https://open.bigmodel.cn/api/paas/v4",
            "model_name": "glm-4.5-air",
            "temperature": 0.7,
            "max_tokens": 8192,
            "timeout": 600,
            "interface_format": "æ™ºè°±"
        }
    },
    "embedding_configs": {
        "OpenAI": {
            "api_key": "${OPENAI_EMBEDDING_API_KEY}",
            "base_url": "https://api.openai.com/v1",
            "model_name": "text-embedding-ada-002",
            "retrieval_k": 4,
            "interface_format": "OpenAI"
        },
        "æ™ºè°±": {
            "api_key": "${ZHIPUAI_API_KEY}",
            "base_url": "https://open.bigmodel.cn/api/paas/v4",
            "model_name": "embedding-2",
            "retrieval_k": 4,
            "interface_format": "æ™ºè°±"
        },
        "SiliconFlow": {
            "api_key": "${SILICONFLOW_API_KEY}",
            "base_url": "https://api.siliconflow.cn/v1",
            "model_name": "BAAI/bge-m3",
            "retrieval_k": 4,
            "interface_format": "SiliconFlow"
        },
        "Gitee AI": {
            "api_key": "${GITEE_AI_API_KEY}",
            "base_url": "https://ai.gitee.com/v1",
            "model_name": "bge-m3",
            "retrieval_k": 4,
            "interface_format": "Gitee AI"
        }
    },
    "other_params": {
        "topic": "",
        "genre": "",
        "num_chapters": 0,
        "word_number": 0,
        "filepath": "./novel_output",
        "chapter_num": "120",
        "user_guidance": "",
        "characters_involved": "",
        "key_items": "",
        "scene_location": "",
        "time_constraint": ""
    },
    "choose_configs": {
        "prompt_draft_llm": "DeepSeek V3",
        "chapter_outline_llm": "DeepSeek V3",
        "architecture_llm": "Gemini 2.5 Pro",
        "final_chapter_llm": "GPT 5",
        "consistency_review_llm": "DeepSeek V3"
    },
    "proxy_setting": {
        "proxy_url": "127.0.0.1",
        "proxy_port": "",
        "enabled": False
    },
    "webdav_config": {
        "webdav_url": "",
        "webdav_username": "",
        "webdav_password": ""
    }
}
    save_config(config, config_file)
    return config


def save_config(config_data: dict, config_file: str) -> bool:
    """å°† config_data ä¿å­˜åˆ° config_file ä¸­ï¼Œæ”¯æŒJSONå’ŒYAMLæ ¼å¼ï¼Œè¿”å› True/False è¡¨ç¤ºæ˜¯å¦æˆåŠŸã€‚"""
    main_logger.info(f"ä¿å­˜é…ç½®æ–‡ä»¶: {config_file}")
    try:
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ ¼å¼
        if config_file.endswith('.yaml') or config_file.endswith('.yml'):
            with open(config_file, 'w', encoding='utf-8') as f:
                yaml.dump(config_data, f, allow_unicode=True, indent=2)
        else:
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        main_logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def test_llm_config(interface_format, api_key, base_url, model_name, temperature, max_tokens, timeout, log_func, handle_exception_func):
    """æµ‹è¯•å½“å‰çš„LLMé…ç½®æ˜¯å¦å¯ç”¨"""
    def task():
        # åˆå§‹åŒ–ä»£ç†å˜é‡
        old_http_proxy = None
        old_https_proxy = None
        old_http_proxy_lower = None
        old_https_proxy_lower = None
        
        try:
            # ä¸´æ—¶æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ä»¥é¿å…è¿æ¥é—®é¢˜
            import os
            old_http_proxy = os.environ.pop('HTTP_PROXY', None)
            old_https_proxy = os.environ.pop('HTTPS_PROXY', None)
            old_http_proxy_lower = os.environ.pop('http_proxy', None)
            old_https_proxy_lower = os.environ.pop('https_proxy', None)

            try:
                log_func("å¼€å§‹æµ‹è¯•LLMé…ç½®...")
                main_logger.info(f"æµ‹è¯•LLMé…ç½® - æ¥å£æ ¼å¼: {interface_format}, æ¨¡å‹: {model_name}")

                llm_adapter = create_llm_adapter(
                    interface_format=interface_format,
                    base_url=base_url,
                    model_name=model_name,
                    api_key=api_key,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=timeout
                )

                test_prompt = "Please reply 'OK'"
                log_llm_request(test_prompt, model_name, interface_format)

                response = llm_adapter.invoke(test_prompt)
                log_llm_response(response, model_name, interface_format)

                if response:
                    log_func("âœ… LLMé…ç½®æµ‹è¯•æˆåŠŸï¼")
                    log_func(f"æµ‹è¯•å›å¤: {response}")
                else:
                    log_func("âŒ LLMé…ç½®æµ‹è¯•å¤±è´¥ï¼šæœªè·å–åˆ°å“åº”")

            finally:
                # æ¢å¤ä»£ç†ç¯å¢ƒå˜é‡
                import os
                if old_http_proxy:
                    os.environ['HTTP_PROXY'] = old_http_proxy
                if old_https_proxy:
                    os.environ['HTTPS_PROXY'] = old_https_proxy
                if old_http_proxy_lower:
                    os.environ['http_proxy'] = old_http_proxy_lower
                if old_https_proxy_lower:
                    os.environ['https_proxy'] = old_https_proxy_lower

        except Exception as e:
            # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿæ¢å¤ä»£ç†è®¾ç½®
            import os
            if old_http_proxy:
                os.environ['HTTP_PROXY'] = old_http_proxy
            if old_https_proxy:
                os.environ['HTTPS_PROXY'] = old_https_proxy
            if old_http_proxy_lower:
                os.environ['http_proxy'] = old_http_proxy_lower
            if old_https_proxy_lower:
                os.environ['https_proxy'] = old_https_proxy_lower

            log_func(f"âŒ LLMé…ç½®æµ‹è¯•å‡ºé”™: {str(e)}")
            handle_exception_func("æµ‹è¯•LLMé…ç½®æ—¶å‡ºé”™")

    threading.Thread(target=task, daemon=True).start()


def test_llm_config_with_dict(config_dict, log_func, handle_exception_func):
    """ä½¿ç”¨é…ç½®å­—å…¸æµ‹è¯•LLMé…ç½®"""
    try:
        # ä»é…ç½®å­—å…¸ä¸­æå–å‚æ•°ï¼Œæä¾›é»˜è®¤å€¼
        interface_format = config_dict.get('interface_format', 'OpenAI')
        api_key = config_dict.get('api_key', '')
        base_url = config_dict.get('base_url', 'https://api.openai.com/v1')
        model_name = config_dict.get('model_name', 'gpt-3.5-turbo')
        temperature = config_dict.get('temperature', 0.7)
        max_tokens = config_dict.get('max_tokens', 8192)
        timeout = config_dict.get('timeout', 300)
        
        # è°ƒç”¨åŸå§‹æµ‹è¯•å‡½æ•°
        test_llm_config(
            interface_format=interface_format,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            log_func=log_func,
            handle_exception_func=handle_exception_func
        )
        return True, "æµ‹è¯•å·²å¯åŠ¨"
    except Exception as e:
        error_msg = f"é…ç½®å‚æ•°é”™è¯¯: {str(e)}"
        log_func(f"âŒ {error_msg}")
        return False, error_msg


def test_embedding_config(api_key, base_url, interface_format, model_name, log_func, handle_exception_func):
    """æµ‹è¯•å½“å‰çš„Embeddingé…ç½®æ˜¯å¦å¯ç”¨"""
    def task():
        # åˆå§‹åŒ–ä»£ç†å˜é‡
        old_http_proxy = None
        old_https_proxy = None
        old_http_proxy_lower = None
        old_https_proxy_lower = None
        
        try:
            # ä¸´æ—¶æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ä»¥é¿å…è¿æ¥é—®é¢˜
            import os
            old_http_proxy = os.environ.pop('HTTP_PROXY', None)
            old_https_proxy = os.environ.pop('HTTPS_PROXY', None)
            old_http_proxy_lower = os.environ.pop('http_proxy', None)
            old_https_proxy_lower = os.environ.pop('https_proxy', None)

            try:
                log_func("å¼€å§‹æµ‹è¯•Embeddingé…ç½®...")
                main_logger.info(f"æµ‹è¯•Embeddingé…ç½® - æ¥å£æ ¼å¼: {interface_format}, æ¨¡å‹: {model_name}")

                # åœ¨æµ‹è¯•å‰æ‰“å°é…ç½®ä¿¡æ¯ç”¨äºè°ƒè¯•
                log_func(f"æµ‹è¯•é…ç½®: interface_format={interface_format}, base_url={base_url}, model_name={model_name}")

                embedding_adapter = create_embedding_adapter(
                    interface_format=interface_format,
                    api_key=api_key,
                    base_url=base_url,
                    model_name=model_name
                )

                test_text = "æµ‹è¯•æ–‡æœ¬"
                log_embedding_request(test_text, model_name, interface_format)

                embeddings = embedding_adapter.embed_query(test_text)
                log_embedding_response(embeddings, model_name, interface_format)

                # æ›´è¯¦ç»†çš„å‘é‡æ£€æŸ¥
                if embeddings is None:
                    log_func("âŒ Embeddingé…ç½®æµ‹è¯•å¤±è´¥ï¼šè¿”å›å€¼ä¸ºNone")
                    main_logger.error("åµŒå…¥æµ‹è¯•å¤±è´¥ - embed_queryè¿”å›None")
                elif isinstance(embeddings, list):
                    if len(embeddings) > 0:
                        # æ£€æŸ¥å‘é‡å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                        valid_count = sum(1 for x in embeddings if isinstance(x, (int, float)) and not str(x).lower() == 'nan')
                        if valid_count == len(embeddings):
                            log_func("âœ… Embeddingé…ç½®æµ‹è¯•æˆåŠŸï¼")
                            log_func(f"ç”Ÿæˆçš„å‘é‡ç»´åº¦: {len(embeddings)}")
                            main_logger.info(f"åµŒå…¥æµ‹è¯•æˆåŠŸ - å‘é‡ç»´åº¦: {len(embeddings)}")
                        else:
                            log_func(f"âŒ Embeddingé…ç½®æµ‹è¯•å¤±è´¥ï¼šå‘é‡åŒ…å«æ— æ•ˆæ•°æ® ({valid_count}/{len(embeddings)} æœ‰æ•ˆ)")
                            main_logger.error(f"åµŒå…¥æµ‹è¯•å¤±è´¥ - å‘é‡æ•°æ®æ— æ•ˆ: {embeddings[:5]}...")
                    else:
                        log_func("âŒ Embeddingé…ç½®æµ‹è¯•å¤±è´¥ï¼šè¿”å›ç©ºå‘é‡")
                        main_logger.error("åµŒå…¥æµ‹è¯•å¤±è´¥ - è¿”å›ç©ºå‘é‡åˆ—è¡¨")
                else:
                    log_func(f"âŒ Embeddingé…ç½®æµ‹è¯•å¤±è´¥ï¼šè¿”å›å€¼ç±»å‹é”™è¯¯ ({type(embeddings)})")
                    main_logger.error(f"åµŒå…¥æµ‹è¯•å¤±è´¥ - è¿”å›å€¼ç±»å‹é”™è¯¯: {type(embeddings)}, å€¼: {str(embeddings)[:200]}")

                # å¦‚æœembeddingsæœ‰é—®é¢˜ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯
                if not embeddings or (isinstance(embeddings, list) and len(embeddings) == 0):
                    log_func("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥APIå“åº”æ ¼å¼...")
                    # å°è¯•ç›´æ¥è°ƒç”¨APIè·å–åŸå§‹å“åº”æ¥è°ƒè¯•
                    try:
                        import requests
                        headers = {"Authorization": f"Bearer {api_key}"}
                        debug_url = f"{base_url}/embeddings"
                        debug_payload = {"input": test_text, "model": model_name}
                        debug_response = requests.post(debug_url, json=debug_payload, headers=headers, timeout=10)
                        if debug_response.status_code == 200:
                            log_func(f"ğŸ” APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(debug_response.text)} å­—ç¬¦")
                            # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦é¿å…æ—¥å¿—è¿‡é•¿
                            response_preview = debug_response.text[:200] + "..." if len(debug_response.text) > 200 else debug_response.text
                            log_func(f"ğŸ” å“åº”é¢„è§ˆ: {response_preview}")
                        else:
                            log_func(f"ğŸ” APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {debug_response.status_code}")
                    except Exception as debug_e:
                        log_func(f"ğŸ” è°ƒè¯•APIè°ƒç”¨å¤±è´¥: {str(debug_e)}")

            finally:
                # æ¢å¤ä»£ç†ç¯å¢ƒå˜é‡
                import os
                if old_http_proxy:
                    os.environ['HTTP_PROXY'] = old_http_proxy
                if old_https_proxy:
                    os.environ['HTTPS_PROXY'] = old_https_proxy
                if old_http_proxy_lower:
                    os.environ['http_proxy'] = old_http_proxy_lower
                if old_https_proxy_lower:
                    os.environ['https_proxy'] = old_https_proxy_lower

        except Exception as e:
            # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿæ¢å¤ä»£ç†è®¾ç½®
            import os
            if old_http_proxy:
                os.environ['HTTP_PROXY'] = old_http_proxy
            if old_https_proxy:
                os.environ['HTTPS_PROXY'] = old_https_proxy
            if old_http_proxy_lower:
                os.environ['http_proxy'] = old_http_proxy_lower
            if old_https_proxy_lower:
                os.environ['https_proxy'] = old_https_proxy_lower

            log_func(f"âŒ Embeddingé…ç½®æµ‹è¯•å‡ºé”™: {str(e)}")
            handle_exception_func("æµ‹è¯•Embeddingé…ç½®æ—¶å‡ºé”™")

    threading.Thread(target=task, daemon=True).start()


def test_embedding_config_with_dict(config_dict, log_func, handle_exception_func):
    """ä½¿ç”¨é…ç½®å­—å…¸æµ‹è¯•Embeddingé…ç½®"""
    try:
        # ä»é…ç½®å­—å…¸ä¸­æå–å‚æ•°ï¼Œæä¾›é»˜è®¤å€¼
        interface_format = config_dict.get('interface_format', 'OpenAI')
        api_key = config_dict.get('api_key', '')
        base_url = config_dict.get('base_url', 'https://api.openai.com/v1')
        model_name = config_dict.get('model_name', 'text-embedding-ada-002')
        
        # è°ƒç”¨åŸå§‹æµ‹è¯•å‡½æ•°
        test_embedding_config(
            api_key=api_key,
            base_url=base_url,
            interface_format=interface_format,
            model_name=model_name,
            log_func=log_func,
            handle_exception_func=handle_exception_func
        )
        return True, "æµ‹è¯•å·²å¯åŠ¨"
    except Exception as e:
        error_msg = f"é…ç½®å‚æ•°é”™è¯¯: {str(e)}"
        log_func(f"âŒ {error_msg}")
        return False, error_msg


def get_zhipu_model_list(api_key, base_url, log_func):
    """è·å–æ™ºè°±AIæ¨¡å‹åˆ—è¡¨"""
    try:
        log_func("æ­£åœ¨è·å–æ™ºè°±AIæ¨¡å‹åˆ—è¡¨...")
        from llm_adapters import ZhipuAIAdapter
        
        # åˆ›å»ºæ™ºè°±AIé€‚é…å™¨å®ä¾‹
        adapter = ZhipuAIAdapter(
            api_key=api_key,
            base_url=base_url,
            model_name="glm-4.5-air",  # ä¸´æ—¶ä½¿ç”¨é»˜è®¤æ¨¡å‹
            max_tokens=1000,
            temperature=0.7,
            timeout=30
        )
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        models = adapter.get_model_list()
        if models:
            log_func(f"âœ… è·å–åˆ° {len(models)} ä¸ªæ¨¡å‹:")
            for model in models:
                log_func(f"  - {model}")
            return models
        else:
            log_func("âŒ æœªè·å–åˆ°æ¨¡å‹åˆ—è¡¨")
            return []
    except Exception as e:
        log_func(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return []